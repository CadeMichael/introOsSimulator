1.  This is a malloc simulation task.  You may choose any language, and any teammate.  Solo project is possible but not advised, considering the two parts required here.  If a team member is a Data Science major or minor, we need detailed documentation of what each person did (it should be clear from the two parts I am requiring, if each student takes one part).

2.  Part 1a.  Requests.  

- Choose n, 10 < n < 1000.  Define a probability req(i) for each i =1..n block of memory that a process might request at any time step of the simulation, if not already allocated.  Define a size(n) for each n, in terms of # of pages, 1 < size(n) < 20.  For each i, j < n, define a metoo(i | j) which is the probability that i will be requested in a step in the simulation, if i is not already allocated, and j has been allocated.  Define a probability free(i) for each i =1..n block of memory that a process will free the block i if it is allocated.  If you want to be fancy, you can use metoo(i | j) for freeing i if j has been freed, but you are not required to.  You should constrain requests to one req() or free() or metoo() per time step of the simulation.  Now generate a well-formed stream of requests (do not request if already allocated, do not free unless allocated).  **I admit that there are many ways to interpret the metoo(i,j) when i is not allocated and several of the j's are -- does one take the max?  the avg?  the  most recently allocated?  This is actually one of the earliest problems in AI/uncertain-reasoning I got to address, so I can confidently say there is no single right answer, and many unadvisable ones (maybe don't sum over all j that have been allocated).  A good thing for you to think about at this stage in life.

3.  Part 2a.  Allocation and Deallocation Algorithm.

- This part takes a stream of requests to allocate or free each block of size(i) memory, as defined by Part 1a.  You can assume you have a large number of available pages, initially a single contiguous block, b, not too big and not too small, e.g., 1000 < b < 10000 pages.  You should maintain a free list of available pages, and ranges of contiguous free pages.  You can choose any policy or algorithm here for satisfying a request.  You could choose buddy blocks for example.  You could choose best-fit or first-fit.  You should log your decisions as output, reporting where you satisfied a request (e.g., "request at t=1738 for i=33, size(i)=12 pages, satisfied by free block of size 32 starting at 8056 leaving 20 pages as 1x16 + 2x2, though you do NOT have to simulate a buddy system).  At each step in the simulation you should report some measure of what is the degree of fragmentation, and the total overhead, if any, for maintaining the free list (for example, maintaining sorted order, or merging buddy blocks).

4.  Part 2b.  Correctness.

- This should be explicit.  The simulator should take in a small sequence of requests that exhibit that it is working correctly.  This requires a report of which pages were taken from the free list to satisfy each request, and how that impacted the free list, as well as showing that the allocation is permissible, correct according to the policy, and that the measurement of fragmentation (however you are choosing to do this) is correct, before and after.

5.  Part 1b.  Evaluation/Interpretation.

- You now run your simulator on a large stream of allocation and free requests.  In the past, when students have chosen results after t<1000, I have penalized and commented that t > 100000 is the min t that I would expect.  You should vary one parameter, such as b, or n, or sum(req(i)) or sum(metoo(i, j)), then run the simulation again, and make some intelligent observation or visualization about the differences observed.  Unless you are unable to satisfy a req(i) for size(i), the observation will likely be in terms of the measurement of fragmentation.  But here, you may be able to surprise us (yes, in the past, students have iterated over a range of values and charted their observations).  I would recommend not fussing over too many details of the simulation but instead thinking about what could be done in terms of the algorithm.  For example, if one had some large sizes yet to be requested, and with  high probability, one would want to maintain a large enough block, even if it meant copying to defragment (on demand, or in anticipatory fashion).  

5.  Have a little fun with this.  The difference between implementing and simulating is that the former is spec-driven design, while the latter is curiosity-driven exploratioin/investigation.  Note that this is only worth 8%, not 10%, 15%, or 20%, so you do not need to tear your hair out.
